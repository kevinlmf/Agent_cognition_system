"""
Memory Systems Comparison
å¯¹æ¯”æˆ‘ä»¬çš„Memoryç³»ç»Ÿä¸å¸¸è§baselineï¼ˆLSTMã€Transformerã€Memory Networksç­‰ï¼‰
"""
import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
evaluation_dir = os.path.dirname(current_dir)
project_root = os.path.dirname(evaluation_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import numpy as np
from typing import Dict, List, Any, Optional
from datetime import datetime
import json

from evaluation.comparison.baseline_memory import (
    LSTMMemory, TransformerMemory, MemoryNetworkBaseline, EpisodicMemoryBaseline
)
from main import CognitiveAgent


class MemoryComparisonEvaluator:
    """
    å¯¹æ¯”è¯„ä¼°å™¨ï¼šå¯¹æ¯”æˆ‘ä»¬çš„Memoryç³»ç»Ÿä¸baseline
    """
    
    def __init__(self):
        self.results = {}
    
    def evaluate_retrieval_accuracy(self, our_memory, baseline_memory, 
                                   test_queries: List[np.ndarray],
                                   ground_truth: List[List[int]]) -> Dict[str, float]:
        """
        è¯„ä¼°æ£€ç´¢å‡†ç¡®ç‡
        """
        our_accuracies = []
        baseline_accuracies = []
        
        for query, true_indices in zip(test_queries, ground_truth):
            # æˆ‘ä»¬çš„ç³»ç»Ÿ
            our_results = our_memory.retrieve(
                query_embedding=query,
                retrieval_strategy="similar",
                top_k=len(true_indices)
            )
            our_retrieved = set([i for i in range(len(our_results))])
            our_accuracy = len(our_retrieved & set(true_indices)) / len(true_indices) if true_indices else 0.0
            our_accuracies.append(our_accuracy)
            
            # Baseline
            baseline_results = baseline_memory.retrieve(query, top_k=len(true_indices))
            baseline_retrieved = set([r.get('index', -1) for r in baseline_results])
            baseline_accuracy = len(baseline_retrieved & set(true_indices)) / len(true_indices) if true_indices else 0.0
            baseline_accuracies.append(baseline_accuracy)
        
        return {
            'our_accuracy': np.mean(our_accuracies),
            'baseline_accuracy': np.mean(baseline_accuracies),
            'improvement': np.mean(our_accuracies) - np.mean(baseline_accuracies)
        }
    
    def evaluate_explicit_query(self, our_memory, baseline_memory, 
                               queries: List[str]) -> Dict[str, Any]:
        """
        è¯„ä¼°æ˜¾å¼æŸ¥è¯¢èƒ½åŠ›ï¼ˆå¦‚"å¯¹æ‰‹1çš„VPIPæ˜¯å¤šå°‘ï¼Ÿ"ï¼‰
        """
        our_success = 0
        baseline_success = 0
        
        for query in queries:
            # æˆ‘ä»¬çš„ç³»ç»Ÿï¼šå¯ä»¥æ˜¾å¼æŸ¥è¯¢
            try:
                if hasattr(our_memory, 'query_memory'):
                    our_result = our_memory.query_memory(query)
                    if our_result:
                        our_success += 1
            except:
                pass
            
            # Baselineï¼šæ— æ³•æ˜¾å¼æŸ¥è¯¢
            baseline_success += 0  # Baselineæ— æ³•åšæ˜¾å¼æŸ¥è¯¢
        
        return {
            'our_explicit_query_rate': our_success / len(queries) if queries else 0.0,
            'baseline_explicit_query_rate': 0.0,
            'explicit_query_advantage': our_success / len(queries) if queries else 0.0
        }
    
    def evaluate_structured_storage(self, our_memory, baseline_memory) -> Dict[str, Any]:
        """
        è¯„ä¼°ç»“æ„åŒ–å­˜å‚¨èƒ½åŠ›
        """
        our_stats = our_memory.get_statistics() if hasattr(our_memory, 'get_statistics') else {}
        baseline_stats = baseline_memory.get_statistics()
        
        # æ£€æŸ¥ç»“æ„åŒ–èƒ½åŠ›
        our_structured = {
            'has_episodic': hasattr(our_memory, 'episodic_memory'),
            'has_semantic': hasattr(our_memory, 'semantic_memory'),
            'has_graph': hasattr(our_memory, 'memory_graph'),
            'can_query_by_type': True  # æˆ‘ä»¬çš„ç³»ç»Ÿå¯ä»¥
        }
        
        baseline_structured = {
            'has_episodic': baseline_stats.get('type') == 'Episodic Memory',
            'has_semantic': False,  # Baselineé€šå¸¸æ²¡æœ‰
            'has_graph': False,  # Baselineé€šå¸¸æ²¡æœ‰
            'can_query_by_type': False  # Baselineæ— æ³•æŒ‰ç±»å‹æŸ¥è¯¢
        }
        
        return {
            'our_structured_features': our_structured,
            'baseline_structured_features': baseline_structured,
            'structured_advantage': sum(our_structured.values()) - sum(baseline_structured.values())
        }
    
    def evaluate_long_term_dependency(self, our_memory, baseline_memory,
                                     long_history: List[np.ndarray]) -> Dict[str, float]:
        """
        è¯„ä¼°é•¿æœŸä¾èµ–èƒ½åŠ›
        """
        # å­˜å‚¨é•¿æœŸå†å²
        for i, obs in enumerate(long_history):
            our_memory.store_experience(
                world_snapshot={'step': i},
                perception_result={'summary': f'Step {i}'},
                reward=i * 0.01,
                embedding=obs
            )
            
            baseline_memory.store(obs, content=f'Step {i}')
        
        # æŸ¥è¯¢æ—©æœŸè®°å¿†
        early_query = long_history[0]
        
        # æˆ‘ä»¬çš„ç³»ç»Ÿ
        our_results = our_memory.retrieve(
            query_embedding=early_query,
            retrieval_strategy='similar',
            top_k=5
        )
        our_can_retrieve_early = len(our_results) > 0
        
        # Baseline
        baseline_results = baseline_memory.retrieve(early_query, top_k=5)
        baseline_can_retrieve_early = len(baseline_results) > 0
        
        return {
            'our_long_term_ability': 1.0 if our_can_retrieve_early else 0.0,
            'baseline_long_term_ability': 1.0 if baseline_can_retrieve_early else 0.0,
            'long_term_advantage': (1.0 if our_can_retrieve_early else 0.0) - (1.0 if baseline_can_retrieve_early else 0.0)
        }
    
    def compare_with_baseline(self, our_memory, baseline_memory, 
                            baseline_name: str,
                            test_scenarios: Dict[str, Any]) -> Dict[str, Any]:
        """
        å…¨é¢å¯¹æ¯”æˆ‘ä»¬çš„ç³»ç»Ÿä¸baseline
        """
        print(f"\n{'='*80}")
        print(f"å¯¹æ¯”: Our Memory System vs {baseline_name}")
        print(f"{'='*80}")
        
        comparison_results = {
            'baseline_name': baseline_name,
            'comparison_timestamp': datetime.now().isoformat()
        }
        
        # 1. æ£€ç´¢å‡†ç¡®ç‡
        if 'test_queries' in test_scenarios and 'ground_truth' in test_scenarios:
            retrieval_results = self.evaluate_retrieval_accuracy(
                our_memory, baseline_memory,
                test_scenarios['test_queries'],
                test_scenarios['ground_truth']
            )
            comparison_results['retrieval_accuracy'] = retrieval_results
            print(f"\nğŸ“Š æ£€ç´¢å‡†ç¡®ç‡:")
            print(f"   æˆ‘ä»¬çš„ç³»ç»Ÿ: {retrieval_results['our_accuracy']:.3f}")
            print(f"   {baseline_name}: {retrieval_results['baseline_accuracy']:.3f}")
            print(f"   æå‡: {retrieval_results['improvement']:+.3f}")
        
        # 2. æ˜¾å¼æŸ¥è¯¢èƒ½åŠ›
        if 'explicit_queries' in test_scenarios:
            explicit_results = self.evaluate_explicit_query(
                our_memory, baseline_memory,
                test_scenarios['explicit_queries']
            )
            comparison_results['explicit_query'] = explicit_results
            print(f"\nğŸ” æ˜¾å¼æŸ¥è¯¢èƒ½åŠ›:")
            print(f"   æˆ‘ä»¬çš„ç³»ç»Ÿ: {explicit_results['our_explicit_query_rate']:.3f}")
            print(f"   {baseline_name}: {explicit_results['baseline_explicit_query_rate']:.3f}")
        
        # 3. ç»“æ„åŒ–å­˜å‚¨
        structured_results = self.evaluate_structured_storage(our_memory, baseline_memory)
        comparison_results['structured_storage'] = structured_results
        print(f"\nğŸ“ ç»“æ„åŒ–å­˜å‚¨:")
        print(f"   æˆ‘ä»¬çš„ç³»ç»Ÿ: {structured_results['our_structured_features']}")
        print(f"   {baseline_name}: {structured_results['baseline_structured_features']}")
        print(f"   ä¼˜åŠ¿: +{structured_results['structured_advantage']}")
        
        # 4. é•¿æœŸä¾èµ–
        if 'long_history' in test_scenarios:
            long_term_results = self.evaluate_long_term_dependency(
                our_memory, baseline_memory,
                test_scenarios['long_history']
            )
            comparison_results['long_term_dependency'] = long_term_results
            print(f"\nâ° é•¿æœŸä¾èµ–:")
            print(f"   æˆ‘ä»¬çš„ç³»ç»Ÿ: {long_term_results['our_long_term_ability']:.3f}")
            print(f"   {baseline_name}: {long_term_results['baseline_long_term_ability']:.3f}")
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        scores = []
        if 'retrieval_accuracy' in comparison_results:
            scores.append(comparison_results['retrieval_accuracy']['improvement'] + 0.5)
        if 'explicit_query' in comparison_results:
            scores.append(comparison_results['explicit_query']['explicit_query_advantage'])
        if 'structured_storage' in comparison_results:
            scores.append(min(1.0, comparison_results['structured_storage']['structured_advantage'] / 4.0))
        
        overall_score = np.mean(scores) if scores else 0.5
        comparison_results['overall_advantage'] = float(overall_score)
        
        print(f"\nğŸ“ˆ ç»¼åˆä¼˜åŠ¿å¾—åˆ†: {overall_score:.3f}")
        
        return comparison_results
    
    def comprehensive_comparison(self, our_memory, baselines: Dict[str, Any],
                                test_scenarios: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸å¤šä¸ªbaselineå…¨é¢å¯¹æ¯”
        """
        print("\n" + "="*80)
        print("MEMORYç³»ç»Ÿå…¨é¢å¯¹æ¯”")
        print("="*80)
        
        all_comparisons = {}
        
        for baseline_name, baseline_memory in baselines.items():
            comparison = self.compare_with_baseline(
                our_memory, baseline_memory, baseline_name, test_scenarios
            )
            all_comparisons[baseline_name] = comparison
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        report = {
            'comparison_timestamp': datetime.now().isoformat(),
            'our_system': 'Our Memory System (Episodic + Semantic + Graph)',
            'baselines_compared': list(baselines.keys()),
            'comparisons': all_comparisons,
            'summary': self._generate_summary(all_comparisons)
        }
        
        self.results = report
        return report
    
    def _generate_summary(self, comparisons: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå¯¹æ¯”æ€»ç»“"""
        avg_advantages = []
        for baseline_name, comp in comparisons.items():
            if 'overall_advantage' in comp:
                avg_advantages.append(comp['overall_advantage'])
        
        return {
            'average_advantage': np.mean(avg_advantages) if avg_advantages else 0.0,
            'best_baseline_comparison': max(comparisons.items(), 
                                           key=lambda x: x[1].get('overall_advantage', 0))[0] if comparisons else None,
            'total_baselines': len(comparisons)
        }
    
    def save_results(self, filepath: str):
        """ä¿å­˜å¯¹æ¯”ç»“æœ"""
        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)
        with open(filepath, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        print(f"\nâœ“ å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {filepath}")


def create_test_scenarios():
    """åˆ›å»ºæµ‹è¯•åœºæ™¯"""
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [np.random.randn(128) for _ in range(10)]
    
    # Ground truthï¼ˆå‡è®¾å‰5ä¸ªæ˜¯æœ€ç›¸å…³çš„ï¼‰
    ground_truth = [[i for i in range(5)] for _ in range(10)]
    
    # æ˜¾å¼æŸ¥è¯¢
    explicit_queries = [
        "å¯¹æ‰‹1çš„VPIPæ˜¯å¤šå°‘ï¼Ÿ",
        "æœ€è¿‘10æ‰‹çš„å¹³å‡åˆ©æ¶¦æ˜¯å¤šå°‘ï¼Ÿ",
        "ç³»ç»Ÿç¨³å®šæ€§æŒ‡æ ‡å¦‚ä½•ï¼Ÿ"
    ]
    
    # é•¿æœŸå†å²
    long_history = [np.random.randn(128) for _ in range(200)]
    
    return {
        'test_queries': test_queries,
        'ground_truth': ground_truth,
        'explicit_queries': explicit_queries,
        'long_history': long_history
    }


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå…¨é¢å¯¹æ¯”"""
    print("="*80)
    print("Memoryç³»ç»Ÿå¯¹æ¯”è¯„ä¼°")
    print("="*80)
    
    # åˆ›å»ºæˆ‘ä»¬çš„Memoryç³»ç»Ÿ
    print("\n1. åˆå§‹åŒ–æˆ‘ä»¬çš„Memoryç³»ç»Ÿ...")
    our_memory = CognitiveAgent(mode="langgraph")
    
    # å¡«å……ä¸€äº›æ•°æ®
    np.random.seed(42)
    for i in range(50):
        obs_text = f"Observation {i}: Market event occurred"
        our_memory.perceive(obs_text, source="test")
        if i % 10 == 0:
            our_memory.record_action(
                action_type="test_action",
                parameters={"step": i},
                result=f"Processed step {i}",
                reward=i * 0.01
            )
    
    # åˆ›å»ºbaselines
    print("\n2. åˆå§‹åŒ–Baseline Memoryç³»ç»Ÿ...")
    baselines = {
        'LSTM': LSTMMemory(hidden_size=128),
        'Transformer': TransformerMemory(d_model=128),
        'Memory Networks': MemoryNetworkBaseline(memory_size=1000),
        'Episodic Memory': EpisodicMemoryBaseline(max_memories=10000)
    }
    
    # å¡«å……baselineæ•°æ®
    np.random.seed(42)
    for i in range(50):
        obs = np.random.randn(128)
        for baseline in baselines.values():
            baseline.store(obs, content=f'Observation {i}')
    
    # åˆ›å»ºæµ‹è¯•åœºæ™¯
    print("\n3. åˆ›å»ºæµ‹è¯•åœºæ™¯...")
    test_scenarios = create_test_scenarios()
    
    # è¿è¡Œå¯¹æ¯”
    print("\n4. è¿è¡Œå¯¹æ¯”è¯„ä¼°...")
    evaluator = MemoryComparisonEvaluator()
    report = evaluator.comprehensive_comparison(
        our_memory.memory_engine,
        baselines,
        test_scenarios
    )
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    evaluator.save_results(f'evaluation_results/memory_comparison_{timestamp}.json')
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*80)
    print("å¯¹æ¯”æ€»ç»“")
    print("="*80)
    summary = report['summary']
    print(f"\nå¹³å‡ä¼˜åŠ¿: {summary['average_advantage']:.3f}")
    print(f"æœ€ä½³å¯¹æ¯”: {summary['best_baseline_comparison']}")
    print(f"å¯¹æ¯”çš„Baselineæ•°é‡: {summary['total_baselines']}")
    
    return report


if __name__ == "__main__":
    main()

