"""
Memory Effectiveness Evaluation Framework
è¯„ä¼°Memoryç³»ç»Ÿåœ¨ä¸åŒç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§

æ”¯æŒçš„ç¯å¢ƒï¼š
- Trading (Stock Market)
- Bitcoin Trading
- FlexSim Simulation
- Poker AI (å¦‚æœå­˜åœ¨)
"""
import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import numpy as np
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime
import json
from dataclasses import dataclass, asdict


@dataclass
class EnvironmentMetrics:
    """ç¯å¢ƒç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡"""
    environment_name: str
    total_episodes: int
    memory_usage: Dict[str, Any]
    decision_quality: Dict[str, float]
    performance_metrics: Dict[str, float]
    memory_impact: Dict[str, float]  # æœ‰memory vs æ— memoryçš„å¯¹æ¯”


class MemoryEffectivenessEvaluator:
    """
    è¯„ä¼°Memoryç³»ç»Ÿåœ¨ä¸åŒç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§
    """
    
    def __init__(self):
        self.results = {}
        self.environment_results = {}
    
    def evaluate_environment(self, 
                           environment_name: str,
                           run_simulation: Callable,
                           baseline_run: Optional[Callable] = None) -> EnvironmentMetrics:
        """
        è¯„ä¼°ç‰¹å®šç¯å¢ƒä¸­çš„memoryæœ‰æ•ˆæ€§
        
        Args:
            environment_name: ç¯å¢ƒåç§°
            run_simulation: è¿è¡Œæ¨¡æ‹Ÿçš„å‡½æ•°ï¼Œè¿”å›agentå’Œç»Ÿè®¡ä¿¡æ¯
            baseline_run: å¯é€‰ï¼Œè¿è¡Œæ— memory baselineçš„å‡½æ•°
        """
        print(f"\n{'='*80}")
        print(f"è¯„ä¼°ç¯å¢ƒ: {environment_name}")
        print(f"{'='*80}")
        
        # è¿è¡Œå¸¦memoryçš„æ¨¡æ‹Ÿ
        print("\n1. è¿è¡Œå¸¦Memoryçš„æ¨¡æ‹Ÿ...")
        agent_with_memory, stats_with_memory = run_simulation()
        
        # æ”¶é›†memoryæŒ‡æ ‡
        memory_stats = self._collect_memory_metrics(agent_with_memory)
        
        # æ”¶é›†å†³ç­–è´¨é‡æŒ‡æ ‡
        decision_quality = self._evaluate_decision_quality(agent_with_memory, stats_with_memory)
        
        # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        performance_metrics = self._extract_performance_metrics(stats_with_memory)
        
        # å¦‚æœæœ‰baselineï¼Œè¿›è¡Œå¯¹æ¯”
        memory_impact = {}
        if baseline_run:
            print("\n2. è¿è¡ŒBaselineï¼ˆæ— Memoryï¼‰æ¨¡æ‹Ÿ...")
            agent_baseline, stats_baseline = baseline_run()
            memory_impact = self._compare_with_baseline(
                stats_with_memory, stats_baseline
            )
        else:
            print("\n2. è·³è¿‡Baselineå¯¹æ¯”ï¼ˆæœªæä¾›baselineå‡½æ•°ï¼‰")
        
        # åˆ›å»ºæŒ‡æ ‡å¯¹è±¡
        metrics = EnvironmentMetrics(
            environment_name=environment_name,
            total_episodes=memory_stats.get('total_episodes', 0),
            memory_usage=memory_stats,
            decision_quality=decision_quality,
            performance_metrics=performance_metrics,
            memory_impact=memory_impact
        )
        
        self.environment_results[environment_name] = metrics
        
        # æ‰“å°ç»“æœ
        self._print_environment_results(metrics)
        
        return metrics
    
    def _collect_memory_metrics(self, agent) -> Dict[str, Any]:
        """æ”¶é›†memoryä½¿ç”¨æƒ…å†µæŒ‡æ ‡"""
        try:
            stats = agent.agent.get_statistics()
            mem_stats = stats.get('memory', {})
            mem_graph = mem_stats.get('memory_graph', {})
            
            # è®¡ç®—memoryåˆ©ç”¨ç‡
            total_memories = mem_graph.get('total_memories', 0)
            episodic_memories = mem_graph.get('episodic_memories', 0)
            semantic_memories = mem_graph.get('semantic_memories', 0)
            
            # Memoryæ£€ç´¢ç»Ÿè®¡
            retrieval_stats = {
                'total_memories': total_memories,
                'episodic_memories': episodic_memories,
                'semantic_memories': semantic_memories,
                'memory_diversity': self._calculate_memory_diversity(agent),
                'memory_consolidation_rate': semantic_memories / max(1, episodic_memories)
            }
            
            return retrieval_stats
        except Exception as e:
            print(f"Warning: Could not collect memory metrics: {e}")
            return {'total_episodes': 0, 'error': str(e)}
    
    def _calculate_memory_diversity(self, agent) -> float:
        """è®¡ç®—memoryå¤šæ ·æ€§ï¼ˆåŸºäºembeddingç›¸ä¼¼åº¦ï¼‰"""
        try:
            memories = agent.agent.memory_engine.memory_graph.memories
            if len(memories) < 2:
                return 0.0
            
            embeddings = []
            for mem in memories.values():
                if mem.embedding is not None:
                    embeddings.append(mem.embedding)
            
            if len(embeddings) < 2:
                return 0.0
            
            # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
            embeddings_array = np.array(embeddings)
            similarities = []
            for i in range(len(embeddings_array)):
                for j in range(i+1, len(embeddings_array)):
                    sim = np.dot(embeddings_array[i], embeddings_array[j]) / (
                        np.linalg.norm(embeddings_array[i]) * 
                        np.linalg.norm(embeddings_array[j])
                    )
                    similarities.append(sim)
            
            avg_similarity = np.mean(similarities) if similarities else 0.0
            diversity = 1.0 - avg_similarity  # å¤šæ ·æ€§ = 1 - å¹³å‡ç›¸ä¼¼åº¦
            
            return float(diversity)
        except Exception as e:
            return 0.0
    
    def _evaluate_decision_quality(self, agent, stats: Dict) -> Dict[str, float]:
        """è¯„ä¼°å†³ç­–è´¨é‡"""
        quality_metrics = {}
        
        # åŸºäºrewardçš„å†³ç­–è´¨é‡
        if 'total_reward' in stats or 'total_profit' in stats:
            reward = stats.get('total_reward', stats.get('total_profit', 0))
            quality_metrics['reward_based_quality'] = max(0, min(1, reward / 1000.0))
        
        # åŸºäºèƒœç‡çš„å†³ç­–è´¨é‡ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        if 'win_rate' in stats:
            quality_metrics['win_rate'] = stats['win_rate']
        
        # åŸºäºreturnçš„å†³ç­–è´¨é‡ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        if 'total_return' in stats:
            quality_metrics['return_based_quality'] = max(0, min(1, (stats['total_return'] + 1) / 2))
        
        # Memoryæ£€ç´¢ç›¸å…³æ€§ï¼ˆå¦‚æœæœ‰ï¼‰
        try:
            state = agent.agent.get_state()
            relevant_memories = len(state.relevant_memories)
            important_memories = len(state.important_memories)
            quality_metrics['memory_relevance'] = min(1.0, (relevant_memories + important_memories) / 20.0)
        except:
            quality_metrics['memory_relevance'] = 0.0
        
        return quality_metrics
    
    def _extract_performance_metrics(self, stats: Dict) -> Dict[str, float]:
        """æå–æ€§èƒ½æŒ‡æ ‡"""
        performance = {}
        
        # é€šç”¨æ€§èƒ½æŒ‡æ ‡
        if 'total_profit' in stats:
            performance['total_profit'] = stats['total_profit']
        if 'total_return' in stats:
            performance['total_return'] = stats['total_return']
        if 'win_rate' in stats:
            performance['win_rate'] = stats['win_rate']
        if 'avg_throughput' in stats:
            performance['avg_throughput'] = stats['avg_throughput']
        if 'total_reward' in stats:
            performance['total_reward'] = stats['total_reward']
        
        return performance
    
    def _compare_with_baseline(self, 
                              stats_with_memory: Dict,
                              stats_baseline: Dict) -> Dict[str, float]:
        """å¯¹æ¯”æœ‰memoryå’Œæ— memoryçš„æ€§èƒ½"""
        impact = {}
        
        # å¯¹æ¯”å„ç§æŒ‡æ ‡
        for key in ['total_profit', 'total_return', 'win_rate', 'total_reward', 'avg_throughput']:
            if key in stats_with_memory and key in stats_baseline:
                with_mem = stats_with_memory[key]
                baseline = stats_baseline[key]
                
                if baseline != 0:
                    improvement = (with_mem - baseline) / abs(baseline)
                    impact[f'{key}_improvement'] = float(improvement)
                else:
                    impact[f'{key}_improvement'] = float('inf') if with_mem > 0 else 0.0
        
        return impact
    
    def _print_environment_results(self, metrics: EnvironmentMetrics):
        """æ‰“å°ç¯å¢ƒè¯„ä¼°ç»“æœ"""
        print(f"\n{'='*80}")
        print(f"ç¯å¢ƒè¯„ä¼°ç»“æœ: {metrics.environment_name}")
        print(f"{'='*80}")
        
        print(f"\nğŸ“Š Memoryä½¿ç”¨æƒ…å†µ:")
        mem_usage = metrics.memory_usage
        print(f"   æ€»è®°å¿†æ•°: {mem_usage.get('total_memories', 0)}")
        print(f"   æƒ…æ™¯è®°å¿†: {mem_usage.get('episodic_memories', 0)}")
        print(f"   è¯­ä¹‰è®°å¿†: {mem_usage.get('semantic_memories', 0)}")
        print(f"   Memoryå¤šæ ·æ€§: {mem_usage.get('memory_diversity', 0):.3f}")
        print(f"   è®°å¿†æ•´åˆç‡: {mem_usage.get('memory_consolidation_rate', 0):.3f}")
        
        print(f"\nğŸ¯ å†³ç­–è´¨é‡:")
        for key, value in metrics.decision_quality.items():
            print(f"   {key}: {value:.3f}")
        
        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        for key, value in metrics.performance_metrics.items():
            print(f"   {key}: {value:.3f}")
        
        if metrics.memory_impact:
            print(f"\nğŸ’¡ Memoryå½±å“ï¼ˆvs Baselineï¼‰:")
            for key, value in metrics.memory_impact.items():
                if isinstance(value, float) and not np.isinf(value):
                    print(f"   {key}: {value:+.2%}")
                else:
                    print(f"   {key}: {value}")
    
    def comprehensive_evaluation(self, environments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ç»¼åˆè¯„ä¼°å¤šä¸ªç¯å¢ƒ
        
        Args:
            environments: ç¯å¢ƒé…ç½®åˆ—è¡¨
                [
                    {
                        "name": "Trading",
                        "run": run_trading_simulation,
                        "baseline": run_trading_baseline  # å¯é€‰
                    },
                    ...
                ]
        """
        print("\n" + "="*80)
        print("MEMORYç³»ç»Ÿæœ‰æ•ˆæ€§ç»¼åˆè¯„ä¼°")
        print("="*80)
        
        all_metrics = []
        
        for env_config in environments:
            name = env_config['name']
            run_func = env_config['run']
            baseline_func = env_config.get('baseline', None)
            
            metrics = self.evaluate_environment(name, run_func, baseline_func)
            all_metrics.append(metrics)
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        overall_score = self._calculate_overall_score(all_metrics)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'evaluation_timestamp': datetime.now().isoformat(),
            'environments_evaluated': [m.environment_name for m in all_metrics],
            'environment_results': {m.environment_name: asdict(m) for m in all_metrics},
            'overall_score': overall_score,
            'summary': self._generate_summary(all_metrics, overall_score)
        }
        
        self.results = report
        
        # æ‰“å°ç»¼åˆæŠ¥å‘Š
        self._print_comprehensive_report(report)
        
        return report
    
    def _calculate_overall_score(self, metrics_list: List[EnvironmentMetrics]) -> Dict[str, float]:
        """è®¡ç®—ç»¼åˆå¾—åˆ†"""
        scores = {
            'memory_utilization': [],
            'decision_quality': [],
            'performance': [],
            'memory_impact': []
        }
        
        for metrics in metrics_list:
            # Memoryåˆ©ç”¨ç‡å¾—åˆ†
            mem_usage = metrics.memory_usage
            utilization_score = (
                min(1.0, mem_usage.get('total_memories', 0) / 100.0) * 0.3 +
                mem_usage.get('memory_diversity', 0) * 0.4 +
                min(1.0, mem_usage.get('memory_consolidation_rate', 0)) * 0.3
            )
            scores['memory_utilization'].append(utilization_score)
            
            # å†³ç­–è´¨é‡å¾—åˆ†
            decision_scores = list(metrics.decision_quality.values())
            if decision_scores:
                scores['decision_quality'].append(np.mean(decision_scores))
            
            # æ€§èƒ½å¾—åˆ†ï¼ˆå½’ä¸€åŒ–ï¼‰
            perf_scores = []
            for key, value in metrics.performance_metrics.items():
                if 'return' in key or 'profit' in key:
                    perf_scores.append(max(0, min(1, (value + 1) / 2)))
                elif 'rate' in key or 'win' in key:
                    perf_scores.append(value)
                else:
                    perf_scores.append(min(1.0, value / 100.0))
            if perf_scores:
                scores['performance'].append(np.mean(perf_scores))
            
            # Memoryå½±å“å¾—åˆ†
            if metrics.memory_impact:
                impact_scores = [v for v in metrics.memory_impact.values() 
                               if isinstance(v, float) and not np.isinf(v)]
                if impact_scores:
                    # è½¬æ¢ä¸º0-1å¾—åˆ†
                    normalized_impacts = [max(0, min(1, (imp + 1) / 2)) for imp in impact_scores]
                    scores['memory_impact'].append(np.mean(normalized_impacts))
        
        # è®¡ç®—å¹³å‡å¾—åˆ†
        overall = {
            'memory_utilization': np.mean(scores['memory_utilization']) if scores['memory_utilization'] else 0.0,
            'decision_quality': np.mean(scores['decision_quality']) if scores['decision_quality'] else 0.0,
            'performance': np.mean(scores['performance']) if scores['performance'] else 0.0,
            'memory_impact': np.mean(scores['memory_impact']) if scores['memory_impact'] else 0.0
        }
        
        # ç»¼åˆå¾—åˆ†
        overall['total'] = np.mean(list(overall.values()))
        
        return overall
    
    def _generate_summary(self, metrics_list: List[EnvironmentMetrics], 
                         overall_score: Dict[str, float]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‘˜è¦"""
        return {
            'total_environments': len(metrics_list),
            'overall_score': overall_score['total'],
            'best_environment': max(metrics_list, key=lambda m: 
                m.performance_metrics.get('total_profit', 
                m.performance_metrics.get('total_reward', 0))).environment_name,
            'memory_effectiveness': 'High' if overall_score['total'] > 0.7 else 
                                   'Medium' if overall_score['total'] > 0.5 else 'Low'
        }
    
    def _print_comprehensive_report(self, report: Dict[str, Any]):
        """æ‰“å°ç»¼åˆæŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ç»¼åˆè¯„ä¼°æŠ¥å‘Š")
        print("="*80)
        
        summary = report['summary']
        overall_score = report['overall_score']
        
        print(f"\nğŸ“Š è¯„ä¼°æ‘˜è¦:")
        print(f"   è¯„ä¼°ç¯å¢ƒæ•°: {summary['total_environments']}")
        print(f"   ç»¼åˆå¾—åˆ†: {overall_score['total']:.2%}")
        print(f"   Memoryæœ‰æ•ˆæ€§: {summary['memory_effectiveness']}")
        print(f"   æœ€ä½³ç¯å¢ƒ: {summary['best_environment']}")
        
        print(f"\nğŸ“ˆ å„ç»´åº¦å¾—åˆ†:")
        print(f"   Memoryåˆ©ç”¨ç‡: {overall_score['memory_utilization']:.2%}")
        print(f"   å†³ç­–è´¨é‡: {overall_score['decision_quality']:.2%}")
        print(f"   æ€§èƒ½è¡¨ç°: {overall_score['performance']:.2%}")
        if overall_score['memory_impact'] > 0:
            print(f"   Memoryå½±å“: {overall_score['memory_impact']:.2%}")
        
        print("\n" + "="*80)
    
    def save_results(self, filepath: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)
        
        with open(filepath, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        print(f"\nâœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {filepath}")


def run_trading_simulation():
    """è¿è¡ŒTradingç¯å¢ƒæ¨¡æ‹Ÿ"""
    import sys
    import os
    import importlib.util
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    # ä½¿ç”¨ç»å¯¹å¯¼å…¥
    spec = importlib.util.spec_from_file_location(
        "trading_agent_example",
        os.path.join(project_root, "examples", "trading_agent_example.py")
    )
    trading_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(trading_module)
    TradingAgent = trading_module.TradingAgent
    
    agent = TradingAgent()
    
    # ç®€åŒ–çš„æ¨¡æ‹Ÿ
    market_events = [
        {"price": 100.0, "volume": 1000, "news": "Market opens steady."},
        {"price": 102.5, "volume": 1500, "news": "Fed announces rate cut."},
        {"price": 105.0, "volume": 2000, "news": "Stock prices surge."},
    ]
    
    for event in market_events:
        agent.perceive_market({"price": event['price'], "volume": event['volume']}, event['news'])
        action = agent.decide_action({"price": event['price']})
        agent.execute_trade(action, event['price'])
    
    stats = agent.agent.get_statistics()
    portfolio_value = agent.get_portfolio_value(market_events[-1]['price'])
    pnl = portfolio_value - agent.initial_balance
    
    return agent, {
        'total_profit': pnl,
        'total_return': pnl / agent.initial_balance,
        'total_reward': sum([t.get('reward', 0) for t in agent.trade_history])
    }


def run_bitcoin_simulation():
    """è¿è¡ŒBitcoinç¯å¢ƒæ¨¡æ‹Ÿ"""
    import sys
    import os
    import importlib.util
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    spec = importlib.util.spec_from_file_location(
        "bitcoin_trading_example",
        os.path.join(project_root, "examples", "bitcoin_trading_example.py")
    )
    bitcoin_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(bitcoin_module)
    BitcoinTradingAgent = bitcoin_module.BitcoinTradingAgent
    
    agent = BitcoinTradingAgent()
    
    market_events = [
        {"price": 45000.0, "volume": 1250.5, "rsi": 45, "news": "Bitcoin consolidates."},
        {"price": 46500.0, "volume": 1800.2, "rsi": 55, "news": "Bitcoin breaks resistance."},
        {"price": 48000.0, "volume": 2200.8, "rsi": 65, "news": "Bitcoin surges."},
    ]
    
    for event in market_events:
        indicators = {"rsi": event['rsi']}
        agent.perceive_market(event['price'], event['volume'], event['news'], indicators)
        action = agent.decide_action(event['price'], indicators)
        agent.execute_trade(action, event['price'])
    
    stats_dict = agent.get_statistics()
    
    return agent, stats_dict


def run_flexsim_simulation():
    """è¿è¡ŒFlexSimç¯å¢ƒæ¨¡æ‹Ÿ"""
    import sys
    import os
    import importlib.util
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    spec = importlib.util.spec_from_file_location(
        "flexsim_example",
        os.path.join(project_root, "examples", "flexsim_example.py")
    )
    flexsim_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(flexsim_module)
    FlexSimAgent = flexsim_module.FlexSimAgent
    
    agent = FlexSimAgent()
    
    current_state = {
        "production_rate": 100.0,
        "queue_length": 30,
        "resource_utilization": 0.6,
        "throughput": 60.0,
        "bottlenecks": []
    }
    
    for step in range(3):
        current_state['queue_length'] += 10
        agent.perceive_system_state(current_state, [f"Step {step+1}"])
        action = agent.decide_optimization(current_state)
        current_state, _ = agent.apply_optimization(action, current_state)
    
    stats_dict = agent.get_statistics()
    
    return agent, stats_dict


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œç»¼åˆè¯„ä¼°"""
    print("="*80)
    print("Memoryç³»ç»Ÿæœ‰æ•ˆæ€§è¯„ä¼°")
    print("="*80)
    
    evaluator = MemoryEffectivenessEvaluator()
    
    # å®šä¹‰è¦è¯„ä¼°çš„ç¯å¢ƒ
    environments = [
        {
            "name": "Stock Trading",
            "run": run_trading_simulation
        },
        {
            "name": "Bitcoin Trading",
            "run": run_bitcoin_simulation
        },
        {
            "name": "FlexSim Simulation",
            "run": run_flexsim_simulation
        }
    ]
    
    # è¿è¡Œç»¼åˆè¯„ä¼°
    report = evaluator.comprehensive_evaluation(environments)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"evaluation_results/memory_effectiveness_{timestamp}.json"
    evaluator.save_results(output_file)
    
    return report


if __name__ == "__main__":
    main()

